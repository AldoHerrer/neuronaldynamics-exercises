\documentclass[12pt]{article}
\usepackage[a4paper]{geometry}
\usepackage{amsmath}
\usepackage[lastexercise]{exercise}

%%%%%%%%%%% Preamble for exercise sheets
\pagestyle{empty}
\usepackage{vmargin}
\setmarginsrb{2.5cm}{2.5cm}{2.5cm}{2.5cm}{0cm}{0cm}{0cm}{0cm}
\setlength{\parindent}{0cm}
\linespread{1.0}
\newcommand{\Title}[1]{\makebox[\textwidth][c]{{\large\scshape{#1}}}\\[5mm]}
\newcommand{\What}[1]{\makebox[\textwidth][c]{\large{#1}}\\[5mm]}
\newcommand{\Who}[1]{{\parbox[t]{\linewidth}{\centering\large{#1}}\\[1.0cm]}}
%%%%%%%%%%%

\begin{document}

\What{Neuronal Dynamics: Python Exercises}
\Who{Professor Wulfram Gerstner\\Laboratory of Computational Neuroscience, EPF Lausanne}
\Title{Hopfield Network}

\begin{Exercise}
To test the theoretical results of the preceding exercises you will now simulate an associative memory network. You will use a model in which neurons are pixels and take the values of -1 (\emph{off}) or +1 (\emph{on}). The network can store a certain number of pixel patterns, which is to be investigated in this exercise. During a retrieval phase, the network is started with some initial configuration and the network dynamics evolves towards the stored pattern (attractor) which is closest to the initial configuration. The dynamics is that of equation:

\begin{equation} \label{dynam}
S_i(t+1) = sgn\left(\sum_j w_{ij} S_j(t)\right)
\end{equation}

In the Hopfield model each neuron is connected to every other neuron (full connectivity). The connection matrix is
\begin{equation*}
 w_{ij} = \frac{1}{N}\sum_{\mu} p_i^\mu p_j^\mu
\end{equation*}
where N is the number of neurons, $p_i^\mu$ is the value of neuron $i$ in pattern number $\mu$ and the sum runs over all patterns from $\mu=1$ to $\mu=P$. This is a simple correlation based learning rule (Hebbian learning). Since it is not a iterative rule it is sometimes called one-shot learning. The learning rule works best if the patterns that are to be stored are random patterns with equal probability for on (+1) and off (-1). In a large networks (N to infinity) the number of random patterns that can be stored is approximately 0.14 times N.

This session will also introduce you to the concept of classes or object oriented code. You can find a tutorial at
\begin{verbatim}
 http://docs.python.org/tutorial/classes.html
\end{verbatim}
Download \texttt{hopfield.py} and the folder \texttt{alphabet/} from book's webpage. Open ipython and type
\begin{verbatim}
 >> import hopfield
\end{verbatim}
The module contains two classes
\begin{itemize}
 \item \verb|hopfield_network| the network itself which contains \emph{methods} to create patterns and run a retrieval phase
 \item \verb|alphabet| which creates pixel patterns out of the gif files contained in \texttt{alphabet/}
\end{itemize}

\newpage

Create an instance of your \verb|hopfield_network| class (here of size $4\times4$ for instance)
\begin{verbatim}
 >> hn = hopfield.hopfield_network(4)
\end{verbatim}
Now you can create and store patterns using
\begin{verbatim}
 >> hn.make_pattern()
\end{verbatim}
(default is one random pattern with half of its pixels \emph{on}) and test whether it is stored with
\begin{verbatim}
 >> hn.run()
\end{verbatim}
which, by defaults, runs the dynamics for the first pattern with no pixel flipped.\\

\Question \label{quest:mem} In ${4\times 4}$ network: 
What is the experimental maximum number of random patterns the network is able to memorize? Store more and more random patterns and test retrieval of some of them. The first few patterns should be stored perfectly, but then the performance gets worse. Does it correspond to the theoretical maximum number of random patterns the network is able to memorize?

\Question  Increase the network size to ${10\times 10}$. Repeat question \ref{quest:mem}

\Question
Store a finite number of random patterns, e.g. 8. How many wrong pixels can the network tolerate in the initial state so that it still settles into the correct pattern?

\Question
Try to store characters as the relevant patterns. How good is the retrieval? What is the reason? (to store characters see the doc string of the method \verb|make_pattern|)

\Question[title=Bonus]
Try one of the preceding points in bigger networks. Try adding a smooth transfer function \textit{g} to the neurons.

\end{Exercise}
\end{document}